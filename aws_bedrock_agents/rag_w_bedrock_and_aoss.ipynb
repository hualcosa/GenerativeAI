{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2dc3fcb-ae4f-48e6-9b1c-71b002e0fe1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAG with Amazon Bedrock Knowledge Base\n",
    "\n",
    "In this notebook we use the information ingested in the Bedrock knowledge base to answer user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d4975",
   "metadata": {},
   "source": [
    "## Import packages and utility functions\n",
    "Import packages, setup utility functions, interface with Amazon OpenSearch Service Serverless (AOSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "85ce61b6-795b-488c-b400-1ac80d355162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "from typing import Dict\n",
    "from urllib.request import urlretrieve\n",
    "# from langchain_aws import ChatBedrock\n",
    "from langchain.llms import Bedrock\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from IPython.display import Markdown, display\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79eb7df4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opensearch-py in /home/ec2-user/anaconda3/envs/bedrock_py39/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /home/ec2-user/anaconda3/envs/bedrock_py39/lib/python3.9/site-packages (from opensearch-py) (1.26.19)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/bedrock_py39/lib/python3.9/site-packages (from opensearch-py) (2.32.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/bedrock_py39/lib/python3.9/site-packages (from opensearch-py) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/bedrock_py39/lib/python3.9/site-packages (from opensearch-py) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/bedrock_py39/lib/python3.9/site-packages (from opensearch-py) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/bedrock_py39/lib/python3.9/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/bedrock_py39/lib/python3.9/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opensearch-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c1ea784-37bc-4a3f-84e3-1047f7e5cfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "SERVICE = 'aoss'\n",
    "\n",
    "# do not change the name of the CFN stack, we assume that the \n",
    "# blog post creates a stack by this name and read output values\n",
    "# from the stack.\n",
    "CFN_STACK_NAME = \"rag-w-bedrock-kb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61c6f5cc-2384-4f18-8add-418b258e8ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def get_cfn_outputs(stackname: str) -> str:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "def printmd(string: str):\n",
    "    display(Markdown(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "326c8d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to talk to OpenSearch\n",
    "\n",
    "# Define queries for OpenSearch\n",
    "def query_docs(query: str, embeddings: BedrockEmbeddings, aoss_client: OpenSearch, index: str, k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert the query into embedding and then find similar documents from AOSS\n",
    "    \"\"\"\n",
    "\n",
    "    # embedding\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # query to lookup OpenSearch kNN vector. Can add any metadata fields based filtering\n",
    "    # here as part of this query.\n",
    "    query_qna = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "            \"vector\": {\n",
    "                \"vector\": query_embedding,\n",
    "                \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # OpenSearch API call\n",
    "    relevant_documents = aoss_client.search(\n",
    "        body = query_qna,\n",
    "        index = index\n",
    "    )\n",
    "    return relevant_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d011b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_context_for_query(q: str, embeddings: BedrockEmbeddings, aoss_client: OpenSearch, vector_index: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a context out of the similar docs retrieved from the vector database\n",
    "    by concatenating the text from the similar documents.\n",
    "    \"\"\"\n",
    "    print(f\"query -> {q}\")\n",
    "    aoss_response = query_docs(q, embeddings, aoss_client, vector_index)\n",
    "    context = \"\"\n",
    "    for r in aoss_response['hits']['hits']:\n",
    "        s = r['_source']\n",
    "        print(f\"{s['metadata']}\\n{s['text']}\")\n",
    "        context += f\"{s['text']}\\n\"\n",
    "        print(\"----------------\")\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf61b1",
   "metadata": {},
   "source": [
    "## Retrieve parameters needed from the AWS CloudFormation stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10051806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aoss_collection_arn=arn:aws:aoss:us-east-1:992382836107:collection/okmgwzpu5dkgddbqiai4\n",
      "aoss_host=okmgwzpu5dkgddbqiai4.us-east-1.aoss.amazonaws.com\n",
      "aoss_vector_index=sagemaker-readthedocs-io\n",
      "aws_region=us-east-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
    "\n",
    "region = outputs[\"Region\"]\n",
    "aoss_collection_arn = outputs['CollectionARN']\n",
    "aoss_host = f\"{os.path.basename(aoss_collection_arn)}.{region}.aoss.amazonaws.com\"\n",
    "aoss_vector_index = outputs['AOSSVectorIndexName']\n",
    "print(f\"aoss_collection_arn={aoss_collection_arn}\\naoss_host={aoss_host}\\naoss_vector_index={aoss_vector_index}\\naws_region={region}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a5e9e",
   "metadata": {},
   "source": [
    "## Setup Embeddings and Text Generation model\n",
    "\n",
    "We can use LangChain to setup the embeddings and text generation models provided via Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf6613d2-aae8-48e5-adfb-0ea7fb75f2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a boto3 bedrock client\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# we will use Anthropic Claude for text generation\n",
    "claude_llm = ChatBedrock(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "                         model_kwargs=dict(temperature=0.5, top_k=250, top_p=1, stop_sequences=[]))\n",
    "\n",
    "# we will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-g1-text-02\", client=bedrock_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0166a",
   "metadata": {},
   "source": [
    "## Interface with Amazon OpenSearch Service Serverless\n",
    "We use the open-source [opensearch-py](https://pypi.org/project/opensearch-py/) package to talk to AOSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5d36f340-81ea-4617-b37d-57bf7669c9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, SERVICE)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': aoss_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e383e23",
   "metadata": {},
   "source": [
    "## Use Retrieval Augumented Generation (RAG) for answering queries\n",
    "\n",
    "Now that we have setup the LLMs through Bedrock and vector database through AOSS, we are ready to answer queries using RAG. The workflow is as follows:\n",
    "\n",
    "1. Convert the user query into embeddings.\n",
    "\n",
    "1. Use the embeddings to find similar documents from the vector database.\n",
    "\n",
    "1. Create a prompt using the user query and similar documents (retrieved from the vector db) to create a prompt.\n",
    "\n",
    "1. Provide the prompt to the LLM to create an answer to the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224f2c4-b725-4f3a-84ac-914c4eba8a94",
   "metadata": {},
   "source": [
    "## Query 1\n",
    "\n",
    "Let us first ask the our question to the model without providing any context, see the result and then ask the same question with context provided using document retrieved from AOSS and see if the answer improves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5e4276c0-e47d-4e76-9ec4-ec647065e71e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59d559b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Anthropic models need the Human/Assistant terminology used in the prompts, \n",
    "# they work better with XML style tags.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "        ('system', \"\"\"Answer the question based only on the information provided in few sentences.\n",
    "                            <context>\n",
    "                            {context}\n",
    "                            </context>\n",
    "                            Include your answer in the <answer></answer> tags. Do not include any preamble in your answer.\"\"\"),\n",
    "        ('human', \"{question}\")\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7e199e62-2728-4ac5-8f93-1052897106b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Start with the query\n",
    "question = \"What versions of XGBoost are supported by Amazon SageMaker?\"\n",
    "\n",
    "# 2. Now create a prompt by combining the query and the context (which is empty at this time)\n",
    "context = \"\"\n",
    "prompt = prompt_template.format_messages(context=context, question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4be3215-3dde-4abd-8c38-45871e63d058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'><b>question=What versions of XGBoost are supported by Amazon SageMaker?<br>answer=<answer>Amazon SageMaker supports XGBoost versions 0.90 and 1.0.</answer></b></span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 3. Provide the prompt to the LLM to generate an answer to the query without any additional context provided\n",
    "response = claude_llm.invoke(prompt).content\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f429bb-050d-4c81-b532-aa5b8e531990",
   "metadata": {},
   "source": [
    "**The answer provided above is incorrect**, as can be seen from the [SageMaker XGBoost Algorithm page](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). The supported version numbers are \"1.0, 1.2, 1.3, 1.5, and 1.7\".\n",
    "\n",
    "Now, let us see if we can improve upon this answer by using additional information that is available to use in the vector database. **Also notice in the response below that the source of the documents that are being used as context is also being called out (the name of the file in the S3 bucket), this helps create confidence in the response generated by the LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "371f86e8-157f-41b0-88a4-59a56f5507c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query -> What versions of XGBoost are supported by Amazon SageMaker?\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_algorithms_tabular_xgboost.html\"}\n",
      "sagemaker                                                                                                                     stable                                                                                                               Filters:                Example               Dev Guide               SDK Guide                                                                                                                                                                       \tUsing the SageMaker Python SDK \tUse Version 2.x of the SageMaker Python SDK    \tAPIs    \tFrameworks    \tBuilt-in Algorithms\tAmazon Estimators \tTabular\tAutoGluon \tCatBoost \tFactorization Machines \tK-Nearest Neighbors \tLightGBM \tLinearLearner \tTabTransformer \tXGBoost     \tText \tTime-series \tUnsupervised \tVision        \tWorkflows    \tAmazon SageMaker Experiments    \tAmazon SageMaker Debugger    \tAmazon SageMaker Feature Store    \tAmazon SageMaker Model Monitor    \tAmazon SageMaker Processing    \tAmazon SageMaker Model Building Pipeline                                                                                                                  sagemaker                                                                                   \t » \tBuilt-in Algorithms » \tTabular » \tXGBoost \t                                                   Edit on GitHub                                                                                               XGBoost¶   The XGBoost (eXtreme Gradient Boosting) is a popular\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_frameworks_xgboost_using_xgboost.html\"}\n",
      "Flexibility - Take advantage of the full range of XGBoost functionality, such as cross-validation support. You can add custom pre- and post-processing logic and run additional code after training.   \tScalability - The XGBoost open source algorithm has a more efficient implementation of distributed training, which enables it to scale out to more instances and reduce out-of-memory errors.   \tExtensibility - Because the open source XGBoost container is open source, you can extend the container to install additional libraries and change the version of XGBoost that the container uses. For an example notebook that shows how to extend SageMaker containers, see Extending our PyTorch containers.       Use XGBoost as a Built-in Algortihm¶   Amazon SageMaker provides XGBoost as a built-in algorithm that you can use like other built-in algorithms. Using the built-in algorithm version of XGBoost is simpler than using the open source version, because you don’t have to write a training script. If you don’t need the features and flexibility of open source XGBoost, consider using the built-in version. For information about using the Amazon SageMaker XGBoost built-in algorithm, see XGBoost Algorithm in the Amazon SageMaker Developer Guide.       Use the Open Source XGBoost Algorithm¶   If you want the flexibility and additional features that it provides, use the SageMaker open source XGBoost algorithm.   For which XGBoost versions are supported, see the AWS documentation.\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_frameworks_xgboost_using_xgboost.html\"}\n",
      "Serve a Model   \tProcess Input   \tGet Predictions   \tProcess Output           \tBring Your Own Model   \tWrite an Inference Script   \tCreate an XGBoostModel Object       \tHost Multiple Models with Multi-Model Endpoints           \tSageMaker XGBoost Classes   \tSageMaker XGBoost Docker Containers             eXtreme Gradient Boosting (XGBoost) is a popular and efficient machine learning algorithm used for regression and classification tasks on tabular datasets. It implements a technique known as gradient boosting on trees, which performs remarkably well in machine learning competitions.   Amazon SageMaker supports two ways to use the XGBoost algorithm:    \tXGBoost built-in algorithm   \tXGBoost open source algorithm           The XGBoost open source algorithm provides the following benefits over the built-in algorithm:   \tLatest version - The open source XGBoost algorithm typically supports a more recent version of XGBoost. To see the XGBoost version that is currently supported, see XGBoost SageMaker Estimators and Models.   \tFlexibility - Take advantage of the full range of XGBoost functionality, such as cross-validation support. You can add custom pre- and post-processing logic and run additional code after training.\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'><b>question=What versions of XGBoost are supported by Amazon SageMaker?<br>answer=<answer>For information about the XGBoost versions that are supported by Amazon SageMaker, see the AWS documentation.</answer></b></span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Start with the query\n",
    "question = \"What versions of XGBoost are supported by Amazon SageMaker?\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(q, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = prompt_template.format_messages(context=context, question=question)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt).content\n",
    "\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1bd68-f61d-4f15-b152-3f9f54305fa8",
   "metadata": {},
   "source": [
    "## Query 2\n",
    "\n",
    "For the subsequent queries we use RAG directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2ffbe92d-5fcd-480d-a239-0c461f61f4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query -> What are the different types of distributed training supported by SageMaker. Give a short summary of each.\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_api_training_distributed.html\"}\n",
      "Distributed Training APIs¶   SageMaker distributed training libraries offer both data parallel and model parallel training strategies. They combine software and hardware technologies to improve inter-GPU and inter-node communications. They extend SageMaker’s training capabilities with built-in options that require only small code changes to your training scripts.\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_api_training_distributed.html\"}\n",
      "Store    \tAmazon SageMaker Model Monitor    \tAmazon SageMaker Processing    \tAmazon SageMaker Model Building Pipeline                                                                                                                  sagemaker                                                                                   \t » \tAPIs » \tDistributed Training APIs \t                                                   Edit on GitHub                                                                                               Distributed Training APIs¶   SageMaker distributed training libraries offer both data parallel and model parallel training strategies. They combine software and hardware technologies to improve inter-GPU and inter-node communications. They extend SageMaker’s training capabilities with built-in options that require only small code changes to your training scripts.\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_api_training_distributed.html\"}\n",
      "They extend SageMaker’s training capabilities with built-in options that require only small code changes to your training scripts.    The SageMaker Distributed Data Parallel Library¶    \tThe SageMaker Distributed Data Parallel Library Overview \tUse the Library to Adapt Your Training Script\tFor versions between 1.4.0 and 1.8.0 (Latest) \tDocumentation Archive     \tLaunch a Distributed Training Job Using the SageMaker Python SDK \tRelease Notes\tSageMaker Distributed Data Parallel 1.8.0 Release Notes \tRelease History               The SageMaker Distributed Model Parallel Library¶    \tThe SageMaker Distributed Model Parallel Library Overview \tUse the Library’s API to Adapt Training Scripts\tVersion 1.11.0, 1.13.0, 1.14.0, 1.15.0 (Latest) \tDocumentation Archive     \tRun a Distributed Training Job Using the SageMaker Python SDK\tConfiguration Parameters for distribution \tRanking Basics without Tensor Parallelism \tPlacement Strategy with Tensor Parallelism \tPrescaled Batch     \tRelease Notes\tSageMaker Distributed Model Parallel 1.15.0 Release Notes \tRelease History                                                                                            Next                         Previous                                                  © Copyright 2023, Amazon                         Revision af4d7949.\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'><b>question=What versions of XGBoost are supported by Amazon SageMaker?<br>answer=The different types of distributed training supported by SageMaker are:\n",
       "<answer>\n",
       "1. Data Parallel Training: SageMaker's Distributed Data Parallel Library allows you to scale your training by distributing the data across multiple GPUs or instances. It improves inter-GPU and inter-node communication to speed up training.\n",
       "2. Model Parallel Training: SageMaker's Distributed Model Parallel Library allows you to scale your training by distributing the model across multiple GPUs or instances. It provides APIs to help you adapt your training script to use model parallelism.\n",
       "</answer></b></span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Start with the query\n",
    "question = \"What are the different types of distributed training supported by SageMaker. Give a short summary of each.\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(question, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = prompt_template.format_messages(context=context, question=question)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt).content\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8024b1f-3f99-406c-be1d-9368cd1440f4",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5444ae8c-0377-46ad-8d4e-2d41f575c289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query -> What advantages does SageMaker debugger provide?\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_amazon_sagemaker_debugger.html\"}\n",
      "SageMaker Debugger provides a set of built-in rules curated by data scientists and engineers at Amazon to identify common problems while training machine learning models. There is also support for using custom rule source codes for evaluation. In the following sections, you’ll learn how to use both the built-in and custom rules while training your model.    Relationship between debugger hook and rules¶   Using SageMaker Debugger is, broadly, a two-pronged approach. On one hand you have the production of debugging data, which is done through the Debugger Hook, and on the other hand you have the consumption of this data, which can be with rules (for continuous analyses) or by using the SageMaker Debugger SDK (for interactive analyses).   The production and consumption of data are defined independently. For example, you could configure the debugging hook to store only the collection “gradients” and then configure the rules to operate on some other collection, say, “weights”. While this is possible, it’s quite useless as it gives you no meaningful insight into the training process. This is because the rule will do nothing in this example scenario since it will wait for the tensors in the collection “gradients” which are never be emitted.   For more useful and efficient debugging, configure your debugging hook to produce and store the debugging data that you care about and employ rules that operate on that particular data. This way, you ensure that the Debugger is utilized to its maximum potential in detecting anomalies.\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_amazon_sagemaker_debugger.html\"}\n",
      "For more useful and efficient debugging, configure your debugging hook to produce and store the debugging data that you care about and employ rules that operate on that particular data. This way, you ensure that the Debugger is utilized to its maximum potential in detecting anomalies. In this sense, there is a loose binding between the hook and the rules.   Normally, you’d achieve this binding for a training job by providing values for both debugger_hook_config and rules in your estimator. However, SageMaker Debugger simplifies this by allowing you to specify the collection configuration within the Rule object itself. This way, you don’t have to specify the debugger_hook_config in your estimator separately.       Using built-in rules¶   SageMaker Debugger comes with a set of built-in rules which can be used to identify common problems in model training, for example vanishing gradients or exploding tensors. You can choose to evaluate one or more of these rules while training your model to obtain meaningful insight into the training process. To learn more about these built in rules, see SageMaker Debugger Built-in Rules.    Pre-defined debugger hook configuration for built-in rules¶   As mentioned earlier, for efficient analyses, it’s important that the debugging data that is emitted by the hook is relevant to the rules used to operate and analyze the data.\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-992382836107/sagemaker.readthedocs.io_en_stable_debugger.html\"}\n",
      "visibility into training jobs of state-of-the-art machine learning models. This SageMaker Debugger module provides high-level methods to set up Debugger configurations to monitor, profile, and debug your training job. Configure the Debugger-specific parameters when constructing a SageMaker estimator to gain visibility and insights into your training job.    Contents   \tDebugger   \tDebugger Rule APIs   \tDebugger Configuration APIs   \tDebugger Configuration APIs for Framework Profiling (Deprecated)              Debugger Rule APIs¶   \t class sagemaker.debugger.get_rule_container_image_uri(region)¶ \tReturn the Debugger rule image URI for the given AWS Region.   For a full list of rule image URIs, see Use Debugger Docker Images for Built-in or Custom Rules.   \tParameters \tregion (str) – A string of AWS Region. For example, 'us-east-1'.    \tReturns \tFormatted image URI for the given AWS Region and the rule container type.    \tReturn type \tstr            \t class sagemaker.debugger.get_default_profiler_rule¶ \tReturn the default built-in profiler rule with a unique name.   \tReturns \tThe instance of the built-in ProfilerRule.\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'><b>question=What versions of XGBoost are supported by Amazon SageMaker?<br>answer=According to the context, SageMaker Debugger provides the following advantages:\n",
       "<answer>\n",
       "- It provides a set of built-in rules curated by data scientists and engineers at Amazon to identify common problems while training machine learning models.\n",
       "- It supports using custom rule source codes for evaluation.\n",
       "- It allows you to configure the debugging hook to produce and store the debugging data that you care about, and employ rules that operate on that particular data, ensuring that the Debugger is utilized to its maximum potential in detecting anomalies.\n",
       "</answer></b></span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Start with the query\n",
    "question = \"What advantages does SageMaker debugger provide?\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(question, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = prompt_template.format_messages(context=context, question=question)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt).content\n",
    "\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_bedrock_py39",
   "language": "python",
   "name": "conda_bedrock_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ac4445fedcc02e0ec010c021cc980cd9c85bdedf3d57447a4cb4e8d37edc5f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
